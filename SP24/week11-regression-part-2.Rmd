---
title: "STAT 240: Linear Regression: Inference"
author: "Cameron Jones"
date: "Spring 2024"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      error = TRUE, fig.height = 4)
library(tidyverse)
library(modelr)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")

theme_set(theme_minimal())

```

\renewcommand{\prob}{\mathsf{P}}
\newcommand{\E}{\mathsf{E}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\SD}{\mathsf{SD}}
\newcommand{\SE}{\mathsf{SE}}

# Overview

## Learning Outcomes

* These lectures will teach you how to:
    - Assess the strength of a linear relationship visually and with the correlation coefficient
    - Understand the linear regression model and evaluate its assumptions
    - Compute a confidence interval for a linear regression coefficient
    - Carry out a hypothesis test for a linear regression coefficient
    - Conceptualize and construct prediction and confidence intervals around a regression line
    
## Preliminaries

* You will need the `modelr` package for these lectures; run `install.packages("modelr")`. (**New package!**) You will also need `tidyverse` which you should have from previous lectures.

* Files to download to `COURSE/lecture/unit11-regression`:
    - `week11-regression-intro.Rmd`
    - `week11-regression-intro.html`
    - "Part 2" files coming soon!
    
* Files to download to `COURSE/data` (*You likely have these from part 1*)
    - `lions.csv`
    - `riley.txt`
    
* Files to download to `COURSE/scripts` (*You likely have these from previous lectures*)
    - `viridis.R`
    - `ggprob.R`
  

# Confidence Intervals for Beta-1

* You might be sick of it at this point, but let's recall our general form for a confidence interval:

$$
\text{Point Estimate } \pm \text{ Quantile Confidence Score * Standard Error of PE}
$$

* Our **point estimate** is our sampled-based single number which estimates the parameter of interest, the true $\beta_1$ from the model.
    - This will be $\hat{\beta}_1 = r * \frac{s_y}{s_x}$.
    
* Our **quantile confidence score** is the $\alpha + \frac{1-\alpha}{2}$ quantile of the **sampling distribution** (which we don't know yet).

* Our **standard error** is a known formula capturing the variability of the point estimate (which we don't know yet).

## Sampling Distribution and Standard Error

* The derivation of the standard error and sampling distribution of $\hat{\beta}_1$ are beyond the scope of this course.

* However, you will be asked to use the results.

> The sampling distribution associated with $\hat{\beta}_1$ is $t(df = n-2)$.

* This means that the quantile confidence score for a **C = 95%** confidence interval for $\hat{\beta}_1$ will be `qt(0.975, df = n - 2)`.

---

* The formula for the standard error of $\hat{\beta}_1$ is known- however, **we will not ask you to compute** this standard error by hand, **you can just use `summary(lm(...))` to obtain it.**

* For the curious, the formula for the standard error of $\hat{\beta}_1$ is:

$$
SE(\hat{\beta}_1) = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2/(n-2)}{\sum_{i = 1}^n (x_i - \overline{x})^2}}
$$

* However, we will often just obtain its value by running `summary()` on an `lm()` object and using our eyes to extract the standard error. 

```{r}
# Same lions data from last week
lions = read_csv("../../data/lions.csv") %>% 
  rename(black = proportion.black)
```

```{r}
lions_model = lm(black ~ age, data = lions)

summary(lions_model)
```

* $SE(\hat{\beta}_1)$ in the above output is `0.008307`, the number to the right of our estimate of the slope, $\hat{\beta}_1 = $ `0.058591` (from the coefficients table in the middle of the output).

## Full Example

* We now have all the pieces to construct our final confidence interval for $\beta_1$.

$$
\hat{\beta}_1 \pm \text{qt(C + (1-C)/2)*(SE from lm() output)}
$$

* By hand, we could calculate:

```{r}
x = lions$age
y = lions$black
C = 0.95 # 95% confidence interval
n = nrow(lions)

r = cor(x, y)
beta_hat_1 = r * sd(y) / sd(x) # We could have also just done coef(lions_model)[1], or used our eyes to get the number 0.058591 from the summary table
se = 0.008307 # copied from lm() output above; don't worry about calculating by hand, just use lm()
moe = qt(C + (1-C)/2, df = n - 2)*se

left = beta_hat_1 - moe
right = beta_hat_1 + moe

c(left, right)
```

* When interpreting a confidence interval for the true slope, it can be very hard to avoid mathematical jargon like "slope" because the parameter of interest is hard to define in real terms, unlike the true average $\mu$ or the true average $p$.

* But we still should make sure to talk about the context of the data.

> We are 95% confident that the true slope of the line relating lions' ages to the percentage of their nose which is black is between 0.042 and 0.076.

*This interval constitutes strong evidence the true slope is not 0; i.e. there is strong evidence for a positive relationship between our two variables! In fact, a 95% confidence interval that doesn't contain some value $\theta_{\text{test}}$ is equivalent to getting a p-value lower than 0.05 for $H_a: \theta = \theta_{\text{test}}$. Note that $\theta$ is just a general way to write any parameter of interest, like $\mu$, $p$, or $\beta_1$.*

*This underlying connection between confidence intervals and p-values means that technically we can use confidence intervals to draw conclusions about hypotheses, but we try to keep them separate in this class.*

# Hypothesis Testing for Beta-1

* Hypothesis testing for $\beta_1$ follows the exact same six-step framework as all other hypothesis tests; we just need to identify a new test statistic whose distribution we know.

> Conduct a hypothesis test to assess the evidence that there is a POSITIVE linear relationship between lion ages and percentage of nose which is black.

## Step 1: Model Statement

$$
Y_i = \beta_0 + \beta_1 * X + \varepsilon_i \text{, for } i = 1,...n
$$

$$
\text{where }\varepsilon_i \sim N(0, \sigma)
$$

* At this point, we check the three linear regression assumptions with a residual plot, just like we would consider BINS for any model involving the binomial distribution.

* Even if the assumptions are violated, you can still continue conducting linear regression - you will still get numbers to output, they just are no longer guaranteed to be good estimates of the "truth".

```{r}
# From last week, from the modelr package: add_residuals(lm_object) adds a column to your dataframe like mutate() called "resid" which contains the residuals
lions_with_residuals = lions %>%  
  add_residuals(lions_model)

ggplot(lions_with_residuals, aes(x = age, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

> **Linearity** is satisfied; the residuals don't show an obvious curve pattern.

> **Normal errors around 0** is satisfied; the residuals tend towards the central line, and they are symmetric.

> **Constant variance** is definitely **violated**. The residuals tend to "fan out", i.e. their variance increases, as X increases.

* Even though we have an assumption violated, we will continue to conduct our linear regression and just make a note at the end that these results may not be valid. 

*This speaks to a larger point about your projects: We don't expect your datasets to perfectly meet the assumptions of whatever model of inference you end up using! We want you to get practice identifying problems with real data, and also get practice running inference on them anyway. It is okay if your data violates the assumptions, or you get a large p-value. We just want you to get the practice computing, interpreting, assessing, et cetera.*

## Step 2: State Hypotheses

* We are interested in the presence of a **postive** linear relationship between the two variables. Hence, we will use a one-sided alternative hypothesis.

* The null hypothesis captures the idea that there is no pattern, no relationship, no difference, et cetera.

$$
H_0: \beta_1 = 0
$$

$$
H_A: \beta_1 > 0
$$

## Step 3: Identify Test Statistic and Sampling Distribution

* We now get into previously uncharted territory; what is the test statistic whose distribution is known for $\beta_1$?

* In fact, it is the common form $\frac{\text{Point Estimate - Null Value}}{\text{Standard Error}}$.

> Our test statistic for a hypothesis test for $\beta_1$ is: $T = \frac{\hat{\beta}_1 - \beta_{1,null}}{SE(\hat{\beta}_1)} \sim t(n-2)$. 

*Note that $\beta_{1,null}$ is often but not always 0.*

## Step 4: Identify Outcomes From Data and Alt. Hyp.

* First, we must calculate our observed value of the test statistic.

* For this, we need the value of the point estimate and standard error, which we can extract from the `summary(lm())` output.

```{r}
summary(lions_model)
```

```{r}
point_estimate = 0.058591 # Equivalent: coef(lions_model)[2]
se = 0.008307

test_stat = (point_estimate - 0)/se

test_stat
```

> Notice this is the **t value** next to our slope estimate and standard error in the coefficients table of the `summary(lm(...))` output!

* Next, we have to identify what our relevant outcomes are based on the alternative hypothesis.

* Because our alternative $H_a: \beta_1 > 0$ has $>$ in it, we take all the area to the **right** of our observed test statistic.

* We happened to **get it right** here, and will be rewarded with a lower p-value.

```{r}
n = nrow(lions)

gt(df = n - 2) +
  geom_vline(xintercept = test_stat)
```

*Note: Visually, the area to the right of that line should be really, really small.*

## Step 5: Calculate p-value

* Calculating the area to the right of our test statistic is done with `pt()` with `lower.tail = FALSE`, or  `1 - pt()`.

```{r}
# reminder: continuous areas, caution to the wind, no need to adjust with a -1 inside the pt()
pt(test_stat, df = n - 2, lower.tail = FALSE)
```

> The p-value given to you by `summary(lm())` is based on the **two-sided** alternative, which is why it is written as $Pr(>|t|)$; e.g. the probability of getting a more extreme outcome than your observed test statistic in either direction.

## Step 6: Interpret in Context

* Recall that a small p-value constitutes strong evidence for $H_a$ in context.

> There is strong evidence for a positive linear relationship between lions' ages and the percentage of their nose which is black (p $\approx$ 0, linear regression).

# Non-Linear Predictors and The Power Law

* We have mentioned previously that regression is an extremely broad field, where modeling methods have been developed for many different situations.

* However, some situations which don't *look like* simple linear regression can actually be transformed to become simple linear regression!

* All we care about is if we can get the form of the model to:

$$
\text{something } = \beta_0 + \beta_1 * \text{something else}
$$

* If the "something" or "something else" ends up having some extra function applied to it, such as an exponent or a logarithm, that is okay, as long as the model has this general form!

---

## Context: Quadratic Example

* For example, consider the following non-linear relationship of $Y$ and $X$:

$$
Y = \beta_0 + \beta_1 * X^2
$$

* Linear regression does **not** apply to the relationship between $Y$ and $X$.

* We can demonstrate this by generating some random data with a known value of $\beta_1$, and showing that `lm(y~x)` misses the mark.

Graphing some data (with a little random noise added) from this relationship shows clear non-linearity.

```{r}
set.seed(20240411)

true_beta_1 = 0.5 # This is the parameter we are going to try to estimate!

quadratic_data = tibble(
  x = runif(100, 0, 10),
  y = 2 + true_beta_1*x^2 + rnorm(100, sd = 2)
)
ggplot(quadratic_data, aes(x, y)) +
  geom_point() +
  geom_smooth(se = F)
```

* If we push through and run a `y~x` linear model anyway, our estimate of the true $\beta_1$ is way off.

```{r}
lm(y~x, quadratic_data) 
true_beta_1
```

* We have shown that $Y$ and $X$ are not candidates for linear regression.

* However, $Y$ and $X^2$ are!

* Instead of looking at it as $X^2$, you can think of it as some new variable, $V$.

$$
V = X^2
$$
* Then, we have:

$$
Y = \beta_0 + \beta_1 * V
$$

* This might feel like cheating, but it is not! $Y$ and $V = X^2$ really do have a linear relationship.

```{r}
quadratic_data = quadratic_data %>% 
  mutate(v = x^2)

ggplot(quadratic_data, aes(v, y)) +
  geom_point() +
  geom_smooth(se = F) # I did not constraint this line to be straight with method = "lm"... it just chooses to be linear because that's the true underlying relationship!
```

```{r}
# Much better!
lm(y ~ v, quadratic_data)
true_beta_1
```


---

* The penalty for making a substitution like this is often **interpretability.**

> When we compute $\hat{\beta}_1$ for a line of the standard form $Y = \beta_0 + \beta_1*X$, we can interpret that as: "When $X$ increases by one unit, we expect $Y$ to increase by $\hat{\beta}_1$."

* This is useful because $X$ is often a real-life quantity that we care about, and we understand its units - dollars, inches, pounds, et cetera, as well as $Y$.

*For example: from the dataset of Riley's age in months vs. height in inches, we interpret $\hat{\beta_1} = 0.25$ as: "When Riley's age increases by 1 month, we expect his height to increase by 0.25 inches."

* However, in the above scenario where we make the substitution,

> For the above quadratic example, we would interpret $\hat{\beta_1}$ as: "When $X^2$ increases by one unit, we expect $Y$ to increase by $\hat{\beta_1}$".

* If $X$ was in dollars, $X^2$ is in dollars squared - not something that is intuitively understood in real terms.

*Note: It is tempting, but NOT true, to say for the quadratic example "When $X$ increases by one unit we expect $Y$ to increase by $\sqrt{\hat{\beta_1}}$.*

---

## Mass vs. Radius & The Power Law

* The above example demonstrates that non-linear relationships between variables may be converted into linear relationships between transformed versions of those variables.

* This concept can be extended more generally to study coefficients of equations which are not initially linear, but may be transformed to be so.

* Recall the **exoplanets** data from week 6. 

```{r}
planets_orig = read_csv("../../data/exoplanet-confirmed_2022-09-12.csv",
                   skip = 124)

planets = planets_orig %>% 
  filter(default_flag == 1) %>%
  select(pl_name, hostname, discoverymethod, disc_year,
         sy_pnum, pl_rade, pl_bmasse) %>%
  rename(planet = pl_name,
         star = hostname,
         method = discoverymethod,
         year = disc_year,
         number = sy_pnum,
         radius = pl_rade,
         mass = pl_bmasse)
```

```{r}
planets %>% 
  select(mass, radius) %>% 
  drop_na() %>% 
  head()
```

* The mass (in "earth masses", such that 2 is "twice Earth's weight) and radius (in "earth radii", such that 2 is "twice Earth's radius") of a planet are often modeled to have the following relationship, called a **power law**:

$$
\text{mass} = C*\text{radius}^\theta
$$

... where $C$ is some **unimportant** constant, and $\theta$, the exponent on the `radius` term, is the **parameter of interest.**

* With just linear regression, we have no way to conduct inference on $\theta$ directly from `mass` and `radius`.

* However, taking the logarithm of both sides (you don't need to know how to do this, just the bigger picture and result here), we can rewrite this equation as:

$$
log(\text{mass}) = log(C * \text{radius}^\theta)
$$

$$
log(\text{mass}) = log(C) + \theta*log(\text{radius})
$$

* Now this is of the form $\text{something} = \beta_0 + \beta_1 * \text{something else}$, where $\theta$ is now in the place of what we call $\beta_1$! 

* Therefore, conducting inference on the slope of the linear regression line with log(mass) as the response and log(radius) as the predictor is equivalent to conducting inference on $\theta$.

## Inference on The Power Law

* It is obvious from the form of the model $\text{mass} = C*\text{radius}^\theta$ we cannot estimate $\theta$ from `mass` and `radius` directly; this is emphasized by a scatter plot of those data.

```{r}
ggplot(planets, aes(radius, mass)) +
  geom_point() +
  geom_smooth(se = F, method = "lm")
```

* The scatter plot even looks "wrong", suggesting these variables are not truly related in a straight line (which they aren't).

* However, if we instead plotted `log10(radius)` and `log10(mass)`, we get a different story:

```{r}
planets %>% 
  ggplot(aes(log10(radius), log10(mass))) +
  geom_point() +
  geom_smooth(se = F, method = "lm")
```

* While there are still some interesting patterns we observe in the data, linear regression is definitely much more applicable here.

* It is equivalent to just transform the axes of the original variable, which we can achieve with `scale_x_log10` and `scale_y_log10`.

```{r}
planets %>% 
  ggplot(aes(radius, mass)) + #notice: original variables
  geom_point() +
  geom_smooth(se = F, method = "lm") +
  scale_x_log10() + # instead of 0...20...40 on the axis, it is now 1, 10, 100 equally spaced
  scale_y_log10()
```

* This is helpful for a potential viewer because the graph is now in terms of the original units that are easily understood; but don't let it mislead you into thinking there is a linear relationship between mass and radius. The linear relationship is still between log(mass) and log(radius).

---

* With this in mind, we can conduct inference on $\theta$ through estimation of $\beta_1$ with `log10(mass)` as `y` and `log10(radius)` as `x`.

```{r}
summary(lm(log10(mass) ~ log10(radius), planets))
```

* A 95% confidence interval for $\theta$ is:

```{r}
beta_hat_1 = 1.90956
se = 0.03806
n = nrow(planets %>% drop_na(radius, mass))

moe = qt(0.975, df = n - 2) * se

c(beta_hat_1 - moe, beta_hat_1 + moe)
```

> We are 95% confident the true value of $\theta$ in the power law relationship $\text{mass} = C*\text{radius}^\theta$ is between 1.83 and 1.98.

*Note that we always want to interpret in context with minimal mathematical notation, but we don't really have any other option here unless you have the astronomical background to explain what $\theta$ means in real terms; which might even be more confusing!*

---

* The p-value given is for the test of $H_0: \theta = 0$ versus $H_a: \theta \neq 0$, and we get significant evidence against $H_0$.

* For practice, let's test $H_0: \theta = 2$ versus $H_a: \theta \neq 2$.

* Our model is $log(mass_i) = \beta_0 + \beta_1 * log(radius_i) + \varepsilon_i \text{, for } i = 1,...n \text{, where }\varepsilon_i \sim N(0, \sigma)$.

* Checking the three linear regression assumptions with a residual plot:

```{r}
power_law_planets = planets %>% 
  drop_na(radius, mass) %>% 
  mutate(lRadius = log10(radius), lMass = log10(mass))

power_law_model = lm(lMass ~ lRadius, data = power_law_planets)

power_law_planets %>% 
  add_residuals(power_law_model) %>% 
  ggplot(aes(lRadius, resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

> Linearity is satisfied; there is no obvious non-linear pattern to the data.

> Normal errors around 0 is (hesitantly) satisfied; the data tends to stay close to the center line. One could argue for an asymmetric skew towards higher positive residuals, but it is not strong enough to make us declare the assumption definitely violated.

> Constant variance is satisfied; the data have roughly the same spread as you move horizontally across the plot, no fanning out or fanning in pattern.

* Recall that our **test statistic** for the slope is $T = \frac{\hat{\beta}_1 - \beta_{1,null}}{SE(\hat{\beta}_1)} \sim t(n-2)$.

```{r}
# Reminder: beta_hat_1 and se taken from the summary(lm()) output in the confidence interval above this.
test_stat = (beta_hat_1 - 2)/se
test_stat
```

* With a two-sided alternative hypothesis test, we want the area of all outcomes more extreme in either direction than our observed test statistic:

```{r}
gt(df = n - 2) +
  geom_vline(xintercept = test_stat) +
  geom_t_fill(df = n - 2, b = test_stat) +
  geom_t_fill(df = n - 2, a = abs(test_stat))
```

* Our test statistic is **negative** (the bound of the left tail, vertical line above), so we can find the area of the left tail with a direct `pt(test_stat, df = n - 2)` and then double it to include the right tail.

```{r}
2 * pt(test_stat, df = n - 2)
```

> We find strong evidence that the true value of $\theta$ in $\text{mass} = C*\text{radius}^\theta$ is not equal to 2.

*Once again note that we always want to interpret in context with minimal mathematical notation, but we don't really have any other option here.*

# Advanced Intervals

* All of our previous confidence intervals have been for a **parameter of interest** from the model.

* However, confidence intervals can also be extended to estimate other useful quantities, such as the response for a new observation, or the expected value of the response for a new observation.

* We will explore each of those ideas superficially in this section. Note that we will not ask you to compute these intervals by hand; we will teach you how to use `predict()` to obtain them.

* We will switch back to the lions dataset for our investigation.

```{r}
lions = read_csv("../../data/lions.csv") %>% 
  rename(black = proportion.black)
```

## Prediction Intervals for a New Observation

> The goal of a prediction interval is to obtain a numeric interval which, given a new observation with known $X$, will contain its $Y$ with known confidence.

> Mathematically, we write this as an interval for $\hat{y} \mid x^*$; read as "y-hat given x-star", meaning we want to predict the y value at a known value of x, which is x-star.

* For example, a lion cub you know nothing else about has just turned 5 years old (its $X$ value). You are asked to provide an interval which contains the percentage of its nose which is black (its $Y$ value) with 95% confidence.

* Just like all other intervals, our interval will follow the general form:

$$
\text{Point Estimate} \pm \text{Quantile Confidence Score} * \text{Standard Error}
$$

* Our **point estimate** here is our best guess of $\hat{y} \mid x^*$; the percentage of the cub's nose which is black, given that the cub is 5 years old.

* We just spent a week studying this! Our best guess comes from plugging 5 in for $X$ into the best fit line.

```{r}
estimates = coef(lm(black ~ age, lions))

beta_hat_0 = estimates[1]
beta_hat_1 = estimates[2]

new_x = 5

prediction = beta_hat_0 + beta_hat_1 * new_x
prediction
```

* A reminder that this is just finding the height of our best-fit line at $x = 5$.

```{r}
ggplot(lions, aes(x = age, y = black)) +
  geom_point() +
  # fullrange = T extends the blue line to the limits of the graph
  geom_smooth(se = FALSE, method = "lm", fullrange = T) +
  geom_point(aes(x=new_x, y=beta_hat_0 + new_x*beta_hat_1), color="red", size=4) + 
  geom_vline(xintercept = new_x, color="red", linetype="dashed") +
  geom_hline(yintercept = prediction, color = "red", linetype = "dashed")
```

* Now we need the sampling distribution and standard error. Just like all other inference associated with linear regression, the sampling distribution is $t(n-2)$.

* However, the standard error is quite complex.

* The reason for this is when we model our data as:

$$
Y_i = \beta_0 + \beta_1 * X + \varepsilon_i \text{, for } i = 1,...n
$$

$$
\text{where }\varepsilon_i \sim N(0, \sigma)
$$

* We have **three** sources of uncertainty in our prediction: we are estimating the true $\beta_0$, we are estimating the true $\beta_1$, and even if we knew those, we assume there is some random noise around the true line from $\varepsilon_i \sim N(0, \sigma)$.

* The formula for the standard error for $y \mid x^*$ is below, for the curious, but we will never ask you about this formula.

$$
s_{\hat{y}^*} = \sqrt{\left((n-2)^{-1} \sum_{i=1}^n(y_i - \hat{y}_i)^2\right)\left(1 + n^{-1} + \frac{(x^* - \overline{x})^2}{\sum_{i = 1}^n (x_i - \overline{x})^2} \right)}
$$

* With a known point estimate, sampling distribution, and formula for the standard error, we could compute this interval by hand.

* However, we will instead lean on the `predict()` command for these intervals.

### `predict()` for Prediction Intervals

* `predict()` requires two things, with an important but optional third.

> The first argument to `predict()` is an `lm()` model object.

> The second argument is a dataframe `newdata` containing the $x$ point(s) you want the predictions for.

* If you just specify these two arguments, it just gives you the single predicted y value for that $x$ point or points.

```{r}
lions_model = lm(black ~ age, lions)

# Note that the predictor name 'age' in newdata has to exactly match the predictor name 'age' in the model object.
predict(lions_model, newdata = tibble(age = 5))

# Note this is the same as what we manually calculated.
prediction
```

* The reason `newdata` has to be a dataframe is two-fold:
    - 1) `lm()` supports more than one predictor in your model, which we would give as more columns, and...
    - 2) `predict()` can give you more than one prediction at a time.
    - `newdata` being a dataframe makes these specifications very easy.
    - Unfortunately, it makes it just a little awkward for just a single predicted value.
    
```{r}
# For example, predicted values from this model for lion ages 1 through 10

predict(lions_model, newdata = tibble(age = 1:10))
```
    

> The third (optional) argument is `interval`, which you can give the value `"prediction"` for a prediction interval, or `"confidence"` for a confidence interval of the regression line (next section).

- Here's where `predict()` becomes very useful; it will compute prediction intervals for us at each of the specified points when we specify `interval = "prediction"`.

```{r}
intervals = predict(lions_model, newdata = tibble(age = 1:13), interval = "prediction")

intervals
```

* It will do this for every point we include in `newdata`; we happened to just be interested in x=5, but it is trivially easy to produce intervals for any point.

> We are 95% confident that the percentage of nose which is black for a lion that is 5 years old will be between 10.5% and 61.9%.

* The prediction intervals are rather wide; but when we look at them overlaid upon the scatterplot, we see why.

```{r}
ggplot(lions, aes(age, black)) +
  geom_point() +
  geom_line(data = as_tibble(intervals), mapping = aes(x = 1:13, y = lwr)) +
  geom_line(data = as_tibble(intervals), mapping = aes(x = 1:13, y = upr)) +
  labs(
    title = "Lion Ages vs. % nose black",
    subtitle = "With Prediction Intervals"
  ) +
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15))
```

* These prediction intervals should cover roughly 95% of the points in our dataset, which these do.

### General Properties of Prediction Intervals

* Prediction intervals tend to be **wide**, because there is a high degree of randomness in the response of just a single observation.

* Furthermore, prediction intervals are **wider** at the **edges of the data** with respect to $X$.
    - It is a little bit hard to see from the above graph, but the prediction intervals "bow in" towards the middle; they are narrower in the center.
    - Here's how I conceptualize this point: we know that our estimated regression line will always go through $(\bar{x}, \bar{y})$, and that is probably very close to the true line. Imagine sticking a pin through the line at that point, and rotating it up and down like a seesaw. A small rotation of a few degrees does not change the height of the line much near the fulcrum/pivot point/center, but it drastically changes the height at points far from the pivot point.

## Confidence Intervals for the True Line's Height at Some X

> The goal of a confidence interval for the true line's height is to obtain a numeric interval which, given some $X$ value, will contain the $Y$ value of the **true underlying line's height, $\beta_0 + \beta_1 X$**, with known confidence.

> Mathematically, we write this as an interval for $E(Y \mid x^*)$; read as "the expectation of Y given x-star", meaning we want to predict the height of the true underlying line at a known value of x, which is x-star.

* Instead of predicting the $Y$ of a single new observation, we are now predicting where the true regression line is.

### Investigation with Simulation

* Remember that our best-fit line that we compute as $\hat{\beta}_0 + \hat{\beta}_1*X$ (and plot with `geom_smooth`) is just an ESTIMATE.

* We can see that there is uncertainty in this estimate by simulating fake random data from a known real line, calculating the coefficients based off that fake random data, and seeing how close our estimated lines are to the real line.

---

* Our linear regression model is pasted below for reference.

$$
Y_i = \beta_0 + \beta_1 * X + \varepsilon_i \text{, for } i = 1,...n
$$

$$
\text{where }\varepsilon_i \sim N(0, \sigma)
$$

* This has four parameters we can control for our simulations:

- Two are obvious; the real $\beta_0$ and $\beta_1$ that we will generate our real data from.

- Two are less obvious: the size of the dataset, $n$, and the variation of the errors, $\sigma$.

* Our process will be to:

1) Generate $n$ random points from $\beta_0 + \beta_1 * X + \varepsilon_i$.
2) Estimate $\hat{\beta}_0$ and $\hat{\beta}_1$ from that fake data; write them down.

- In our simulation, we can also control **how many times** we repeat this process; a parameter we will call **B**.

```{r}
simulateBestFitLines = function(B = 100, real_beta_0 = 5, real_beta_1 = 10, error_sigma = 5, n = 50) {

# Set up an empty dataframe in which to store our estimates
estimates = tibble(beta_hat_0 = numeric(0), beta_hat_1 = numeric(0))

# Do the following B times:
for (i in 1:B) {
  # Generate n random points from the real beta 0, real beta 1, and error variation
  simulated_data = tibble(
    x = runif(n, 0, 10),
    y = real_beta_0 + real_beta_1 * x + rnorm(n, sd = error_sigma)
  )
  # Compute estimates beta hat 0 and beta hat 1 from that fake data, write them down
  estimates[i,] = t(as_tibble(coef(lm(y ~ x, simulated_data))))
}

# unfortunately geom_abline() doesn't take slope and intercept as variable aesthetics, so I am forced to plot segments instead
estimates_for_plotting = estimates %>% 
  mutate(x = 0,
         xend = 10,
         y = beta_hat_0,
         yend = beta_hat_0 + beta_hat_1 * 10)

# Plotting code
ggplot(estimates_for_plotting) +
  # The estimated lines
  geom_segment(aes(x=x, xend=xend, y=y, yend=yend)) +
  # The real line
  geom_abline(slope = real_beta_1, intercept = real_beta_0, color = "red", linewidth = 3, linetype = "dashed") +
  labs(
    title = paste0(B, " estimates of true line: ", real_beta_0, " + ", real_beta_1, "*X"),
    subtitle = paste0("With n = ", n, ", error variation = ", error_sigma)
  ) +
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15))
}
```

```{r}
simulateBestFitLines(error_sigma = 10)
```

* In this way, we have created a sort of confidence interval for the true regression line; not based in any statistical theory, but the range of these lines would be a great guess, and in fact will be very close to the true form derived through theory.

* As we introduce more variation into the data, our estimates become a little more wilder, but still tend to capture the real line. 

```{r}
simulateBestFitLines(error_sigma = 20)
```

* This graph especially emphasizes the idea that, just like prediction intervals, confidence intervals for the location of the true regression line get **tighter near the middle of the data** and **wider near the edge of the data**.

* Finally, just for giggles, let's see what happens when the error variation $\sigma = 50$ way outpaces the real $\hat{\beta}_1 = 10$:

```{r}
# I encourage you to run this many times in your console and play with the values of the parameters: real_beta_0, real_beta_1, n, error_sigma, and B!
simulateBestFitLines(error_sigma = 50)
```

* I would argue the regression line still does an admirable job capturing the real line despite significant noise!

### A note on `geom_smooth(se = T)`

* The entire semester, we have been ignoring the "transparent gray ribbon" around `geom_smooth`'s line when we leave the `se` argument as `TRUE`, its default.

* That ribbon is actually just many thin confidence intervals for the true regression line!

* Back to the lions data:

```{r}
ggplot(lions, aes(age, black)) +
  geom_point() +
  geom_smooth(method = "lm")
```


### Standard Error of Confidence Regression Intervals

* Like prediction intervals, we will use `predict()` to calculate confidence intervals for the regression line. 

* We could hypothetically calculate them through the general form:

$$
\text{Point Estimate} \pm \text{Quantile Confidence Score} * \text{Standard Error}
$$

* Our point estimate for the regression line at a point is the same for the point estimate of a prediction interval; our best guess is still the location of our blue estimated best-fit line.

* Our quantile confidence score is still `qt(C + (1-C)/2, df = n - 2)`.

* However, the sampling error,

$$
s_{\hat{y}} = \sqrt{\left((n-2)^{-1} \sum_{i=1}^n(y_i - \hat{y}_i)^2\right)\left(n^{-1} + \frac{(x^* - \overline{x})^2}{\sum_{i = 1}^n (x_i - \overline{x})^2} \right)}
$$

* It is subtle, but this error is **smaller** than that of the prediction interval - it only has two sources of error, our estimates of $\hat{\beta}_0$ and $\hat{\beta}_1$.
    - Since we are predicting $E(Y|X^*)$, we apply an expected value to everything in $\hat{\beta}_0 + \hat{\beta}_1*X_i + \varepsilon_i$, and $E(\varepsilon_i)$ is 0, so that whole term goes away and does not contribute to the error.
    - Once again, the math is not important to this course, but the final result that confidence intervals for the regression line are **tighter** than prediction intervals is important.

### `predict()` for Confidence Regression Intervals

* `predict()` for confidence regression intervals works the same as it does for prediction intervals, except we use `interval = "confidence"` instead of `interval = "prediction"`.

```{r}
# More discussion on predict() can be found in the prediction intervals section; I present this code assuming you have seen that section before

intervals_confidence = predict(lions_model, newdata = tibble(age = 1:13), interval = "confidence")

intervals_confidence
```


```{r}
ggplot(lions, aes(age, black)) +
  geom_point() +
  geom_line(data = as_tibble(intervals), mapping = aes(x = 1:13, y = lwr)) +
  geom_line(data = as_tibble(intervals), mapping = aes(x = 1:13, y = upr)) +
  geom_line(data = as_tibble(intervals_confidence), mapping = aes(x = 1:13, y = lwr), color = "darkgreen") +
  geom_line(data = as_tibble(intervals_confidence), mapping = aes(x = 1:13, y = upr), color = "darkgreen") +
  geom_smooth(method = "lm") +
  labs(
    title = "Lion Ages vs. % nose black",
    subtitle = "With Prediction Intervals in Black, Confidence Regression Interval in Green"
  )  +
  theme(plot.title = element_text(size = 20),
        plot.subtitle = element_text(size = 15))
```



